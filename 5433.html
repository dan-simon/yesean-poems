<html>
<head>
    <meta charset='UTF-8'>
</head>
<body>
    English: The edges of the network / observed once / try to correct
    <br/>
    Yesean: yu edega garapa / loki so doya napa / jae reja
    <br/>
    Literal English meaning: of graph edges / [number of times] one -ness perceive / try correct
    <br/>
    Author: yu buye tisu
    <br/>
    Hi, people interested in network reconstruction! This blog does not actually have a lot of information about that, but I have done some work on it, so email me for more info. My email is on the index.html page of this repository.
    <br/>
    First line: two terms from English, "edega" meaning "edge" (not as boundery, but as in math), and "garapa" meaning "network/graph". Second line: strange line structure, P + so + H + C. Also strange at first glance how loki takes a chubeni and so needs "doya" (one) to become a noun. Third line: "jae" (meaning "try to") is a hisato, not a chubeni. "correct" here is a verb, which can also mean "fix" or "mend".
    <br/>
    Poem meaning: I'm writing a short presentation on network reconstruction, so I can basically copy-and-paste my presentation and give a shortened version of that to you as a post. Here it is:
    <br/>
    Network reconstruction is the ability to make predictions about which edges are in a network given a (possibly erroneous) observation. Here's an example. Suppose we have some studies showing that certain interactions between medicines have dangerous side effects and others don’t, but some of our results might be wrong. We can consider a network where vertices are medicines and edges between vertices are bad interactions, and use network reconstruction to make predictions about which of our results are most likely wrong.
    <br/>
    A stochastic block model of a network is a partition of the nodes in the network into subsets and, for each pair of subsets, a connection probability. One can generate networks from a stochastic block model by, for each pair of nodes, connecting them with probability the connection probability between their subsets and leaving them unconnected otherwise.
    <br/>
    To do network reconstruction, given a network, we can try to find stochastic block models that fit it well and then see which edges not in the actual network are likely given these models, or which edges in the actual network are unlikely given these models. We can then guess that these edges were observed incorrectly (since our model says they should be different).
    <br/>
    We can define the entropy of a network given a stochastic block model to be the negative log of the probability of the network being generated using the model. We can make slight changes to the model in each of many steps and then accept or reject each change based on whether it lower the entropy, thus getting a sampling of likely stochatic block models which we can use to find weird edges. Of course this whole method only works if the network can be viewed as generated by a stochastic block model in the first place. If we know nothing about the network, we can’t identify wrong parts of our observations.
    <br/>
    There’s a simplification which allows us to consider, rather than the entropy of a stochastic block model (partition plus probabilities), the entropy of a partition (automatically determining the best partition block model given the partition). Also, due to the way we’re using Monte-Carlo sampling, it’s actually more efficient to get the difference between entropies of two very similar partitions. Unfortunately, due to the two complications in the previous slide, the formula for entropy is a mess. It’s the sum of five terms each of which is itself the sum of a few terms. It takes a while to understand it (or any part of it) by staring at it, so I’m not including it. But it's swap_contribution in https://github.com/samcoolidge/network if you want to look at it.
    <br/>
    The above was all prior work we built on. We made two main changes: we parallelized the algorithm to allow it to run more quickly, and we generalized the algorithm to work not only with undirected networks but also with directed networks.
    <br/>
    One clear thing to parallelize is the Monte-Carlo sampling, which takes most of the time. Instead of having one CPU going through a bunch of partition block models, we can have lots of CPUs each going through a bunch of partition block models and combine all our partition block models at the end. So if before we could get 1000 samples in some amount of time t, now with n CPUs we could get 1000n samples in t time, or 1000 samples in t/n time. This was especially useful on NYU’s HPC cluster where we could run a program on dozens of CPUs, but even on a personal computer it can offer a factor of 4 (or 8) speedup.
    <br/>
    Before sampling, there’s a step called “equilibration” in which we start with a partition with each node in its own group and move nodes around (in the same way as in sampling) until we get to a reasonably-low-entropy partition block model. The models in this step can’t be used since they’re very high-entropy. All equilibration is good for is the partition block model at the end, which we can use as a starting point for sampling. Perhaps surprisingly, equilibration can take a significant amount of time (especially when sampling is parallelized), so it would be nice to parallelize it.
    <br/>
    Equilibration can’t be fully parallelized, since it depends on having enough sequential node moves to get from the starting partition to a low-entropy partition. We instead let every node try to equilibrate and let all the nodes start sampling with the partition from the equilibration of the first node to finish equilibration. The above improvement speeded equilibration up by a factor of 3 or so. The partitions generated in sampling fortunately had enough time to diverse and become distinct enough from each other that the results weren’t much worse.
    <br/>
    With more than two to three CPUs (normal computers often have at least four), our parallel implementation was quicker than the original. (The factor of 2-3 comes from the original using C (fast) and us using Cython (a bit slower). Also, we made some minor algorithmic changes that speeded the code up, especially on networks with lots of edges.)
    <br/>
    Moving on to directed graphs, there were various changes that had to be made. The definition of a partition block model needed to be slightly changed. This was not that hard. Also, the formula for entropy needed to change a lot. No individual term was that hard to figure out, but debugging was very difficult, since if we got the wrong result for entropy change we couldn’t tell where it came from and we had to look at every term. There were other minor changes (e.g., when looking for incorrectly predicted edges iterating over ordered pairs of vertices rather than unordered pairs), but they weren’t that difficult. Overall (apart from the entropy change) adapting the code for directed graphs was rather easy.
    <br/>
    There are some possibilities for further research. The algorithm didn’t do as well for directed graphs as for undirected graphs, and it’s unclear why. Maybe more testing will help. Also, the algorithm can probably be generalized easily to other types of graphs (e.g., bipartite graphs). There are probably also more small optimization opportunities.
    <br/>
    OK, that's the presentation. Let's see how long this post is. Apparently a little less than 8 kilobytes, which is a good post length.
    <br/>
    Tags: @cs, @programming, @network, @graph, @presentation, @post
</body>
</html>
